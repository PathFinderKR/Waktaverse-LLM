{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Importing Libraries",
   "metadata": {
    "collapsed": false
   },
   "id": "f21a95a142b71e93"
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "\n",
    "# huggingface\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-09T06:33:22.830044Z",
     "start_time": "2024-06-09T06:33:20.824473Z"
    }
   },
   "id": "108874429856aabe",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": "# Hyperparameters",
   "metadata": {
    "collapsed": false
   },
   "id": "b452e9ab5ed33883"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@dataclass\n",
    "class CONFIG:\n",
    "    # Debug\n",
    "    debug: bool = False\n",
    "    \n",
    "    # Model\n",
    "    model_id: str = \"PathFinderKR/Waktaverse-Llama-3-KO-8B-Instruct\"\n",
    "    \n",
    "    # Device\n",
    "    device: str = None\n",
    "    \n",
    "    # Tokenizer parameters\n",
    "    max_length: int = 8192\n",
    "    padding: str = \"do_not_pad\"\n",
    "    truncation: bool = True\n",
    "    \n",
    "    # Generation parameters\n",
    "    num_return_sequences: int = 1\n",
    "    max_new_tokens: int = 1024\n",
    "    do_sample: bool = True\n",
    "    temperature: float = 0.6\n",
    "    top_p: float = 0.9\n",
    "    repetition_penalty: float = 1.1\n",
    "    \n",
    "    # bitsandbytes parameters\n",
    "    load_in_4bit: bool = True\n",
    "    bnb_4bit_compute_dtype: torch.dtype = torch.bfloat16\n",
    "    bnb_4bit_quant_type: str = \"nf4\"\n",
    "    bnb_4bit_use_double_quant: bool = True\n",
    "    \n",
    "    # Seed\n",
    "    seed: int = 101"
   ],
   "id": "491e45cb19ec5f1d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Reproducibility",
   "id": "87cdb2a7cd509812"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f\"Seed: {seed}\")\n",
    "    \n",
    "set_seed(CONFIG.seed)"
   ],
   "id": "75475b5c5e64a59a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Device",
   "id": "2f29fda50194a3fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cuda:0\n"
     ]
    }
   ],
   "execution_count": 3,
   "source": [
    "def configure_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        num_gpu = torch.cuda.device_count()\n",
    "        print(\"> Running on GPU\", end=' | ')\n",
    "        print(\"Num of GPUs: \", num_gpu)\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"> Running on MPS\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"> Running on CPU\")\n",
    "    return device\n",
    "\n",
    "CONFIG.device = configure_device()"
   ],
   "id": "5555b5512cd1a1dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Implementation = flash_attention_2\n"
     ]
    }
   ],
   "execution_count": 4,
   "source": [
    "def configure_attn_implementation(device):\n",
    "    if device == \"cuda\":\n",
    "        if torch.cuda.get_device_capability()[0] >= 8: # Ampere, Ada, or Hopper GPUs\n",
    "            attn_implementation = \"flash_attention_2\"\n",
    "            torch_dtype = torch.bfloat16\n",
    "        else:\n",
    "            attn_implementation = \"eager\"\n",
    "            torch_dtype = torch.float16\n",
    "    else:\n",
    "        attn_implementation = \"eager\"\n",
    "        torch_dtype = torch.float32\n",
    "    return attn_implementation, torch_dtype\n",
    "\n",
    "CONFIG.attn_implementation, CONFIG.torch_dtype = configure_attn_implementation(CONFIG.device)"
   ],
   "id": "f0e72bfaf9471e1e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hugging Face",
   "id": "98126af063c251e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /home/pathfinder/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "execution_count": 2,
   "source": [
    "load_dotenv()\n",
    "login(\n",
    "    token=os.getenv(\"HUGGINGFACE_TOKEN\"),\n",
    "    add_to_git_credential=True\n",
    ")"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "14f31dc8d02b7233"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T06:33:24.427221Z",
     "start_time": "2024-06-09T06:33:23.965833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG.model_id)\n",
    "streamer = TextStreamer(tokenizer)"
   ],
   "id": "3a1ea98f6eba58f8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T06:33:24.431915Z",
     "start_time": "2024-06-09T06:33:24.428200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=CONFIG.load_in_4bit,\n",
    "    bnb_4bit_compute_dtype=CONFIG.bnb_4bit_compute_dtype,\n",
    "    bnb_4bit_quant_type=CONFIG.bnb_4bit_quant_type,\n",
    "    bnb_4bit_use_double_quant=CONFIG.bnb_4bit_use_double_quant\n",
    ")"
   ],
   "id": "be26d2efc49fcbf6",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG.model_id,\n",
    "    device_map=CONFIG.device,\n",
    "    attn_implementation=CONFIG.attn_implementation,\n",
    "    torch_dtype=CONFIG.torch_dtype,\n",
    "    quantization_config=quantization_config\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-09T06:33:34.942300Z",
     "start_time": "2024-06-09T06:33:24.433021Z"
    }
   },
   "id": "f5f879cefa518232",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b49311f753584d679edc75df110a6858"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T06:33:34.949474Z",
     "start_time": "2024-06-09T06:33:34.944071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(model)\n",
    "print(f\"Number of parameters: {model.num_parameters() / 1e9:.2f}\")"
   ],
   "id": "9fedd479c0a753a0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "```LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaFlashAttention2(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n    )\n    (norm): LlamaRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n)```"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference",
   "id": "12b7e2fc01311961"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T06:33:34.966297Z",
     "start_time": "2024-06-09T06:33:34.959592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Llama-3 Template\n",
    "def prompt_template(system, user):\n",
    "    return (\n",
    "        \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "        f\"{system}<|eot_id|>\"\n",
    "        \n",
    "        \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        f\"{user}<|eot_id|>\"\n",
    "        \n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    )"
   ],
   "id": "ead7127be980dcd7",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T06:33:34.974426Z",
     "start_time": "2024-06-09T06:33:34.967610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Llama-2 Template\n",
    "def prompt_template_2(system, user):\n",
    "    return (\n",
    "        \"<s>[INST]<<SYS>>\\n\"\n",
    "        f\"{system}\\n\"\n",
    "        \"<</SYS>>\\n\\n\"\n",
    "        \n",
    "        f\"{user}[/INST]\"\n",
    "    )"
   ],
   "id": "531764790a33d41c",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T06:33:34.981830Z",
     "start_time": "2024-06-09T06:33:34.975478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_response(system ,user):\n",
    "    prompt = prompt_template(system, user)\n",
    "    \n",
    "    input_ids = tokenizer.encode(\n",
    "        prompt,\n",
    "        max_length=CONFIG.max_length,\n",
    "        padding=CONFIG.padding,\n",
    "        truncation=CONFIG.truncation,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(CONFIG.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        num_return_sequences=CONFIG.num_return_sequences,\n",
    "        max_new_tokens=CONFIG.max_new_tokens,\n",
    "        do_sample=CONFIG.do_sample,\n",
    "        temperature=CONFIG.temperature,\n",
    "        top_p=CONFIG.top_p,\n",
    "        repetition_penalty=CONFIG.repetition_penalty,\n",
    "        streamer=streamer\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=False)"
   ],
   "id": "1c849786ee0282d8",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T06:33:34.991688Z",
     "start_time": "2024-06-09T06:33:34.983055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "system_prompt = \"다음 지시사항에 대한 응답을 작성해 주세요.\"\n",
    "user_prompt = \"피보나치 수열에 대해 설명해주세요.\""
   ],
   "id": "36dd54c1250609de",
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "response = generate_response(system_prompt, user_prompt)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-09T06:33:45.375657Z",
     "start_time": "2024-06-09T06:33:34.992984Z"
    }
   },
   "id": "aac19ad24cb1cca2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "다음 지시사항에 대한 응답을 작성해 주세요.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "피보나치 수열에 대해 설명해주세요.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "피보나치 수열은 수학에서 자주 사용되는 수열 중 하나로, 0과 1로 시작하여 다음 항이 이전 두 항의 합으로 구성됩니다. 피보나치 수열은 유명한 수학자 레온 알렉산드로비치 피보나치가 제안했으며, 그의 이름을 따서 명명되었습니다. 이 수열은 자연수와 정수를 포함하며, 각 항은 이전 두 항의 합입니다. 예를 들어, 첫 번째 항은 0이고 두 번째 항은 1이며, 세 번째 항은 2이고 네 번째 항은 3입니다. 피보나치 수열은 순차적으로 증가하는 특징이 있지만, 숫자가 커질수록 점점 더 빠르게 증가합니다. 피보나치 수열은 다양한 분야에서 사용되며, 수학, 컴퓨터 과학, 생물학 등에서 중요한 역할을 합니다.<|eot_id|>\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "다음 지시사항에 대한 응답을 작성해 주세요.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "피보나치 수열에 대해 설명해주세요.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "피보나치 수열은 수학에서 자주 사용되는 수열 중 하나로, 0과 1로 시작하여 다음 항이 이전 두 항의 합으로 구성됩니다. 피보나치 수열은 유명한 수학자 레온 알렉산드로비치 피보나치가 제안했으며, 그의 이름을 따서 명명되었습니다. 이 수열은 자연수와 정수를 포함하며, 각 항은 이전 두 항의 합입니다. 예를 들어, 첫 번째 항은 0이고 두 번째 항은 1이며, 세 번째 항은 2이고 네 번째 항은 3입니다. 피보나치 수열은 순차적으로 증가하는 특징이 있지만, 숫자가 커질수록 점점 더 빠르게 증가합니다. 피보나치 수열은 다양한 분야에서 사용되며, 수학, 컴퓨터 과학, 생물학 등에서 중요한 역할을 합니다.<|eot_id|>\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
